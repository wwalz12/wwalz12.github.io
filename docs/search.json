[
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization 1",
    "section": "",
    "text": "This dataset contains information on artists, including their names, genders, birth years, and places of origin. Here, I created a visualization of the top 5 artist birth years to find the years that produced the most artists. The bar chart at the bottom displays the five most common years of birth in the dataset.\nAccessibility Note: All data visualizations in this report include alternative text (alt text) descriptions to ensure the findings are accessible to users with screen readers.\n\n\nlibrary(tidyverse)\n\nartists &lt;- read_csv(\"artists.csv\")\n\ntop_birth_years &lt;- artists |&gt;\n  filter(!is.na(yearOfBirth)) |&gt;\n  group_by(yearOfBirth) |&gt;\n  summarize(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  head(5)\n\ntop_birth_years\n\n# A tibble: 5 × 2\n  yearOfBirth     n\n        &lt;dbl&gt; &lt;int&gt;\n1        1936    49\n2        1930    48\n3        1928    45\n4        1967    45\n5        1938    41\n\n\n\nggplot(top_birth_years, aes(x = reorder(as.factor(yearOfBirth), n), y = n)) +\n  geom_col(fill = \"steelBlue\") +\n  labs(\n    title = \"Top 5 Years of Artist Births\",\n    x = \"Year of Birth\",\n    y = \"Number of Artists\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nData Sources:\n\nThe TidyTuesday data can be found at: TidyTuesday 2021-01-12\nOriginal data source: Tate Art Museum Collection"
  },
  {
    "objectID": "chocolate.html",
    "href": "chocolate.html",
    "title": "Data Visualization 2",
    "section": "",
    "text": "This analysis uses the TidyTuesday chocolate ratings dataset which contains reviews many different chocolate bars. I focus on the countries of cocoa bean origin to see which regions appear most often in the data. The plot below shows the ten most common bean origins.\nAccessibility Note: All data visualizations in this report include alternative text (alt text) descriptions to ensure the findings are accessible to users with screen readers.\n\nlibrary(tidyverse)\n\nchocolate &lt;- read_csv(\"chocolate.csv\")\n\ntop_bean_countries &lt;- chocolate |&gt;\n  filter(!is.na(country_of_bean_origin)) |&gt;\n  count(country_of_bean_origin, sort = TRUE) |&gt;\n  head(10)\n\ntop_bean_countries\n\n# A tibble: 10 × 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n\n\n\nggplot(top_bean_countries, aes(x = reorder(country_of_bean_origin, n), y = n)) +\n  geom_col(fill = \"chocolate4\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Countries of Cocoa Bean Origin\",\n    x = \"Country of Bean Origin\",\n    y = \"Number of Reviews\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nData Sources:\n\nThe TidyTuesday data can be found at: TidyTuesday 2022-01-18\nOriginal data source: Flavors of Cacao Chocolate Database (by way of Georgios and Kelsey)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William Walz",
    "section": "",
    "text": "Hi, this is for my Data Science class! I’ve made two visualizations of random TidyTuesday datasets for Project 1, and two more visualizations for Project 2."
  },
  {
    "objectID": "text-analysis.html",
    "href": "text-analysis.html",
    "title": "Project 2: Text Analysis",
    "section": "",
    "text": "Overview\nI analyze the titles of the Federalist Papers to see if some authors used longer or shorter titles. I analyze the titles of the Federalist Papers to see if some authors used longer or shorter titles.\nAccessibility Note: All data visualizations in this report include alternative text (alt text) descriptions to ensure the findings are accessible to users with screen readers.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\n\ntitles &lt;- read_csv(\"title_author.csv\")\n\nhead(titles)\n\n\n# A tibble: 6 × 4\n  paper author   disputed title                                                 \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                                                 \n1     1 Hamilton no       FEDERALIST No. I. General Introduction                \n2     2 Jay      no       FEDERALIST No. II. Concerning Dangers from Foreign Fo…\n3     3 Jay      no       FEDERALIST No. III. The Same Subject Continued (Conce…\n4     4 Jay      no       FEDERALIST No. IV. The Same Subject Continued (Concer…\n5     5 Jay      no       FEDERALIST No. V. The Same Subject Continued (Concern…\n6     6 Hamilton no       FEDERALIST No. VI. Concerning Dangers from Dissension…\n\n\n\n\nShow code\ntitles_clean &lt;- titles |&gt;\n  mutate(\n    clean_title = str_replace_all(title, \"^FEDERALIST No\\\\. [IVXLC]+\\\\.\\\\s*\", \"\"),\n    clean_title = str_replace_all(clean_title, \"\\\\([^)]*\\\\)\", \"\"),\n    clean_title = str_replace_all(clean_title, \"[[:punct:]]\", \"\"),\n    title_length = str_length(clean_title)\n  )\n\nby_author &lt;- titles_clean |&gt;\n  group_by(author) |&gt;\n  summarise(\n    n_papers = n(),\n    mean_chars = mean(title_length, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(mean_chars))\n\nby_author\n\n\n# A tibble: 3 × 3\n  author   n_papers mean_chars\n  &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;\n1 Madison        29       59.3\n2 Hamilton       51       44.2\n3 Jay             5       31.2\n\n\nTo measure the true length of the titles, I performed data cleaning to remove standard formatting. This involved stripping out the “FEDERALIST No.” prefix with its Roman numerals, removing parenthetical text (usually used for citations), and deleting all punctuation. This ensures the character count reflects the author’s actual descriptive words rather than the document structure.\nThis table summarizes the average cleaned title length (in characters) by author. Madison’s titles are longest on average, while Hamilton and Jay tend to use slightly shorter titles. Madison had 29 papers, Hamilton 51, and Jay 5.\n\n\nShow code\ntitles_clean_copy &lt;- titles_clean\ntitles_starting_with_the &lt;- titles_clean |&gt;\n  select(\n    clean_title, author\n  ) |&gt;\n  mutate(\n    begins_with_the = ifelse(str_detect(clean_title, \"^The(?=\\\\b)\"), 1, 0)\n  ) |&gt;\n  group_by(author\n  ) |&gt;\n  summarize(avg_use_the = mean(begins_with_the) * 100)\n\ntitles_starting_with_the\n\n\n# A tibble: 3 × 2\n  author   avg_use_the\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Hamilton        80.4\n2 Jay             80  \n3 Madison         69.0\n\n\nHere I found titles that began with “The” using str_detect(*). The positive lookahead (?=\\b) made sure that the “The” was matched as a whole word at the start.\n\n\nShow code\nggplot(by_author, aes(x = author, y = mean_chars)) + \n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    x = \"Author\",\n    y = \"Average Number of Characters Per Title\",\n    title = \"Average Federalist Paper Title Length by Author\",\n    subtitle = \"Characters after removing Federalist, prefixes, parentheses\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis chart visualizes the differences in average title length across authors, and puts into perspective the table from by_author. Madison shows the highest mean character count, suggesting that he may have had a preference for longer or more descriptive titles. Jay has the lowest mean count, with Hamilton in the middle of the two.\n\n\nShow code\nggplot(titles_starting_with_the, aes(x = author, y = avg_use_the)) +\n  geom_col(fill = \"darkgreen\", width = 0.75) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    x = \"Author\",\n    y = \"Percent of Titles beginning with 'The'\",\n    title = \"Percent of Titles beginning with 'The' by Author\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the share of each author’s titles that start with “The.” Differences are small, particularly between Hamilton and Jay (with Hamilton having the highest by a very tiny margin), but the pattern displays variation in how authors write their titles.\nData: Project Gutenberg\nSource: Nicholas J. Horton, FederalistPapers"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Project 3: Permutation Test on Life Expectancy (Asia vs Africa)",
    "section": "",
    "text": "Overview\nI test whether countries in Asia and Africa differ in their average life expectancy using data from the Gapminder dataset. I focus on the year 2007 so that each country is represented once (and because that’s the most recent in this dataset). My goal is to see if the observed difference between the two continents could have happened by random chance, or if it reflects a real difference in population health.\nNull hypothesis (H₀): There is no difference in mean life expectancy between Asia and Africa in 2007.\nAlternative hypothesis (Hₐ): The mean life expectancy differs between Asia and Africa in 2007.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(purrr)\n\nkeep_2007 &lt;- gapminder |&gt;\n  filter(year == 2007, continent == \"Asia\" | continent == \"Africa\") |&gt;\n  select(country, continent, lifeExp) |&gt;\n  drop_na()\n\n\nIn the code above, I load the necessary packages and filter the Gapminder dataset to include only countries from Asia and Africa in the year 2007. I then keep only the country name, continent, and life expectancy columns to make a clean dataset for my analysis (remove missing values and focus only on the variables relevant to my research question).\n\n\nVisualize this relationship\n\n\nShow code\nggplot(keep_2007, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot(alpha=0.67) +\n  labs(\n    title = \"Life Expectancy by Continent (2007)\",\n    x = \"Continent\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  guides(fill = \"none\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nAbove is the original data, in boxplots that show life expectancy distributions for the two continents in 2007. Each box is representing the spread of life expectancy within that continent. The plot shows that most Asian countries tend to have higher life expectancies than African countries, but there’s some overlap between the groups as well.\n\n\nObserved Difference (Asia - Africa)\nWhy? In a permutation test, you calculate the difference in means to create a null distribution that represents what you would expect if there was not an actual difference in the groups.\n\n\nShow code\n# first, summary by continent\n\nkeep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  )\n\n\n# A tibble: 2 × 3\n  continent ave_life med_life\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Africa        54.8     52.9\n2 Asia          70.7     72.4\n\n\nShow code\n# observed difference (asia - africa)\n\nobserved_diffs &lt;- keep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  ) |&gt;\n  summarize(\n    obs_ave_diff = diff(ave_life),\n    obs_med_diff = diff(med_life)\n  )\n\nobserved_diffs\n\n\n# A tibble: 1 × 2\n  obs_ave_diff obs_med_diff\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         15.9         19.5\n\n\nHere, I calculate the observed difference in both mean and median life expectancy between Asia and Africa. This step gives me the actual statistic I’m testing against. In this case, the mean difference is 15.92 years and median difference is 19.47 years. These values prove that, on average, people in Asia live substantially longer than those in Africa. In a permutation test, these observed differences act as benchmarks to see how extreme they are relative to differences generated under random shuffling (the null hypothesis).\n\n\nNull sampling distribution\nWhat does this do? It creates the null sampling distribution, whcih shows what differences I’d expect if continent made no real difference tolife expectancy. The function shuffle_sim randomly shuffles the continent labels, recalculates the mean and median (lifeExp) for the shuffled groups, and returns the differences. I repeated this 3,000 times with map to simulate what random differences might look like if there were not a relationship between continent and life expectancy.\n\n\nShow code\n#shuffle_sim\nshuffle_sim &lt;- function(rep, data) {\n  data |&gt;\n    select(continent, lifeExp) |&gt;\n    mutate(continent_perm = sample(continent, replace = FALSE)) |&gt;\n    group_by(continent_perm) |&gt;\n    summarize(\n      ave_perm = mean(lifeExp),\n      med_perm = median(lifeExp),\n      .groups = \"drop\"\n    ) |&gt;\n    summarize(\n      mean_diff_sim = diff(ave_perm),\n      median_diff_sim = diff(med_perm)\n    ) |&gt;\n    mutate(rep = rep)\n}\n\n# null distribution\nset.seed(123)\npermut_stats &lt;- map(1:3000, shuffle_sim, data = keep_2007) |&gt;\n  list_rbind()\n\npermut_stats |&gt; head()\n\n\n# A tibble: 6 × 3\n  mean_diff_sim median_diff_sim   rep\n          &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;\n1          3.29            4.06     1\n2          3.40            6.40     2\n3          1.94            3.20     3\n4         -4.32           -7.19     4\n5          2.38            4.13     5\n6         -2.55           -1.45     6\n\n\n\n\nVisualize! (observed ave diff)\n\n\nShow code\n#observed ave diff\npermut_stats |&gt;\n  ggplot(aes(x = mean_diff_sim)) +\n  geom_histogram(bins = 47) +\n  geom_vline(xintercept = observed_diffs$obs_ave_diff, color = \"red\") +\n               labs(\n                 title = \"Permutation Null Distribution (Mean Difference: Asia - Africa)\",\n                 x = \"Permuted mean differences\",\n                 y = \"Count\"\n               ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThe histogram above shows the null distribution of mean differences generated from the permutations. Each bar represents one possible difference that could occur by random chance if continent didn’t matter. The red line shows the observed mean difference from the real data (15.9 years). Because the red line is far to the right of almost all simulated values, the observed difference is extremely unlikely to be due to chance which provides strong evidence that the continents differ in mean life expectancy (Reject null hypothesis that (lifeExp) in Asia and Africa are not the same).\n\n\nVisualize! (observed med diff)\n\n\nShow code\n#observed med diff\npermut_stats |&gt;\n  ggplot(aes(x = median_diff_sim)) +\n  geom_histogram(bins = 47) +\n         geom_vline(xintercept = observed_diffs$obs_med_diff, color = \"red\") +\n         labs(\n           title = \"Permutation Null Distribution (Median Difference: Asia - Africa)\",\n           x = \"Permuted median differences\",\n           y = \"Count\"\n         ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThis second histogram shows the null distribution of median differences. Again, the red vertical line shows the observed difference in medians (19.5 years). Like the mean comparison, this value lies far beyond the range of most simulated outcomes which suggests that the difference in median life expectancy between Asia and Africa is also statistically meaningful and not arbitrary\n\n\np-value\n\n\nShow code\np_results &lt;- permut_stats |&gt;\n  summarize(\n    p_value_ave = mean(mean_diff_sim &gt; observed_diffs$obs_ave_diff),\n    p_value_med = mean(median_diff_sim &gt; observed_diffs$obs_med_diff)\n  )\n\np_results\n\n\n# A tibble: 1 × 2\n  p_value_ave p_value_med\n        &lt;dbl&gt;       &lt;dbl&gt;\n1           0           0\n\n\nThe p-value shows how likely it is to see a difference this large just by chance. A small p-value means the result is unlikely under the null hypothesis, suggesting a real difference between Asia and Africa.\n\n\nConclusion\nIn this project, I used a permutation simulation to test whether Asia and Africa differ in mean and median life expectancy. After 3,000 random label shuffles, the observed differences were much larger than any values from the null distribution. The p-values were 0, suggesting that the observed gap in life expectancy is absolutely not due to chance. This supports the conclusion that people in Asia generally live longer than those in Africa.\n\n\nData Sources and References:\nGapminder Foundation. (n.d.). Gapminder data. Gapminder Foundation. Retrieved from Gapminder Foundation"
  },
  {
    "objectID": "AmazonBias.html",
    "href": "AmazonBias.html",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "",
    "text": "In 2014, Amazon attempted to automate the talent acquisition process by creating a machine learning algorithm to score job applicants (Dastin, 2018). The company aimed to increase efficiency by developing a tool that could evaluate 100 resumes and identify the top five candidates for recruitment. However, the engineering team eventually disbanded the project in 2015 because the model displayed a systematic bias against women (Dastin, 2018). The tool penalized resumes containing the word “women’s” (e.g., “women’s chess club captain”) and downgraded graduates from two all-women’s colleges (Dastin, 2018)."
  },
  {
    "objectID": "policingdata.html",
    "href": "policingdata.html",
    "title": "Project 5: Analzying US Traffic Stops",
    "section": "",
    "text": "Using SQL and R, this project explores how traffic stop data is structured across three U.S. agencies in the Stanford Open Policing Project (SOPP) database (Pierson et al., 2020). My objective is descriptive: I summarize what the dataset records rather than inferring motivations or making causal claims behind the policing practices.\nI begin by examining how the racial composition of recorded stops in New York State changes over time. Next, I compare Long Beach and San Bernardino to demonstrate how different agencies categorize stop types. Finally, I analyze search and frisk rates in Florida to see how they differ across racial categories while accounting for ambiguous or missing values.\n\nRacial Distribution of Traffic Stops Over Time: New York State\nTo begin the analysis, I examine how the racial composition of recorded traffic stops in New York State changed between 2010 and 2018. The SQL query below groups stops by race and year and counts the number of stops in each category. Because the total number of enforcement actions varies across years, I convert these counts into percentages in R to allow for consistent comparison.\n\n\nShow code\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dbplyr)\n\n# Connect to database\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\n\nShow code\nSELECT\n  subject_race AS race,\n  COUNT(*) AS n_stops,\n  YEAR(date) AS year\nFROM ny_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY race, year\nHAVING year BETWEEN 2010 AND 2018;\n\n\nVisualization: Trends in Recorded Stops\n\n\nShow code\nlibrary(ggplot2)\nlibrary(bit64)\nlibrary(dplyr)\nlibrary(scales)\n\nny_stops_clean &lt;- ny_stops |&gt;\n  mutate(across(where(bit64::is.integer64), as.integer))\n\nny_stops_pct &lt;- ny_stops_clean |&gt;\n  group_by(year) |&gt;\n  mutate(\n    total_stops_year = sum(n_stops),\n    pct = n_stops / total_stops_year\n  ) |&gt;\n  ungroup()\n\nggplot(ny_stops_pct, aes(x = year, y = pct, color = race)) +\n  geom_line(linewidth = 1.1) +\n  geom_point(size = 2) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Percent of Traffic Stops in New York State by Race and Year\",\n    x = \"Year\",\n    y = \"Percent of Stops\",\n    color = \"Race\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nTrends in Racial Comparison\nThe figure shows how the racial composition of recorded traffic stops in New York State changed between 2010 and 2018. Across all observed years, the majority of stops in this dataset involved individuals classified as white, although that share gradually declined from roughly 78% in 2010 to around 70% by 2018. Conversely, stops involving Black and Hispanic drivers show slight increases over the same period; Black stops rose from about 10% to approximately 15%, and Hispanic stops increased from roughly 6% to about 9%.\nThese percentages reflect the distribution within the stop dataset itself, not the demographics of the total population or the driving population. Therefore, the trends describe how the administrative record has shifted, rather than proving a change in policing strategy or population behavior.\n\n\nComparing Stop Types: Long Beach vs. San Bernardino\nFor the second part of the analysis, I compare how different agencies categorize traffic stops. Both Long Beach and San Bernardino report stop types, but neither provides detailed violation codes. To compare them efficiently, I use a SQL UNION to combine the datasets directly in the query. This approach allows me to standardize the city labels before bringing the data into R.\n\n\nShow code\n(SELECT\n  type AS stop_type,\n  COUNT(*) AS type_count,\n  'Long Beach' AS city\nFROM ca_long_beach_2020_04_01\nGROUP BY stop_type)\n\nUNION\n\n(SELECT\n  type AS stop_type,\n  COUNT(*) AS type_count,\n  'San Bernardino' AS city\nFROM ca_san_bernardino_2020_04_01\nGROUP BY stop_type);\n\n\nVisualization: Comparing Stop Type Distributions Across Two Agencies\n\n\nShow code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n# Clean up NAs for visualization\ncombined_types &lt;- ca_stops_combined |&gt;\n  mutate(across(where(bit64::is.integer64), as.integer)) |&gt;\n  mutate(\n    stop_type = ifelse(is.na(stop_type) | stop_type == \"\", \"NA/Unknown\", stop_type)\n  ) |&gt;\n  group_by(city) |&gt;\n  mutate(stop_type = reorder(stop_type, type_count)) |&gt;\n  ungroup()\n\nggplot(combined_types, aes(x = stop_type, y = type_count)) +\n  geom_col(fill = \"#4C72B0\") +\n  coord_flip() +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(~ city, nrow = 1, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of Stop Types Across Two California Agencies\",\n    subtitle = \"Long Beach vs. San Bernardino (SOPP Data)\",\n    x = \"Stop Type\",\n    y = \"Number of Stops\"\n    ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nReporting Conventions by Agency\nThe two agencies report broadly similar structures in their stop type distributions: both record overwhelmingly more vehicular stops than any other category. This consistency aligns with the expectation that traffic stop databases primarily capture driver interactions. However, Long Beach reports a noticeably larger number of stops classified as “pedestrian” compared with San Bernardino.\nThis difference does not necessarily imply different enforcement intensity. Rather, the variation may reflect differences in population density or reporting conventions. Additionally, both agencies include stops labeled “NA” or with empty strings. Because the SOPP documentation notes that some agencies merge multiple non-vehicular stop types (Pierson et al., 2020), the “NA” category likely captures a mixture of poorly coded data rather than a specific type of stop.\n\n\nSearch and Frisk Rates by Race: Florida\nIn the final section, I analyze how often police searches and frisks occur during traffic stops in Florida. The dataset includes flags for search_conducted and frisk_performed. Before computing rates, I categorize ambiguous racial categories (NA, “unknown”, “other”) into a single group to prevent misinterpreting missing data as a substantive demographic finding.\n\n\nShow code\nSELECT\n  CASE\n    WHEN subject_race IS NULL OR subject_race IN ('other', 'unknown', '') THEN 'unknown/other'\n    ELSE subject_race\n  END AS race,\n  COUNT(*) AS total_stops,\n  SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) AS num_searches,\n  SUM(CASE WHEN frisk_performed = 1 THEN 1 ELSE 0 END) AS num_frisk,\n  1.0 * SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) / COUNT(*) AS search_rate,\n  1.0 * SUM(CASE WHEN frisk_performed = 1 THEN 1 ELSE 0 END) / COUNT(*) AS frisk_rate\nFROM fl_statewide_2020_04_01\nGROUP BY race;\n\n\nVisualization: Disparities in Post-Stop Outcomes\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(tidyr)\n\nfl_search_long &lt;- fl_search |&gt;\n  mutate(across(where(bit64::is.integer64), as.integer)) |&gt;\n  select(race, search_rate, frisk_rate) |&gt;\n  mutate(race = factor(race)) |&gt;\n  pivot_longer(\n    cols = c(search_rate, frisk_rate),\n    names_to = \"metric\",\n    values_to = \"rate\"\n  ) |&gt;\n  mutate(\n    metric = recode(\n      metric,\n      \"search_rate\" = \"Search Rate\",\n      \"frisk_rate\"  = \"Frisk Rate\"\n    )\n  )\n\nggplot(fl_search_long,\n       aes(x = race, y = rate, fill = race)) +\n  geom_col() +\n  facet_wrap(~ metric, ncol = 2, scales = \"free_x\") +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 0.01)) +\n  labs(\n    title = \"Search and Frisk Rates by Race in Florida\",\n    x = \"Race\",\n    y = \"Share of Stops Involving a Search or Frisk\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.spacing.x = unit(3, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nAnalysis of Search and Frisk Rates\nIn the Florida dataset, searches occur more frequently than frisks, though both events are rare relative to the total number of stops. Frisk rates are particularly low, generally falling below 0.5% for all groups.\nDespite the low overall frequency, the data indicates that Black drivers experience the highest rates of both searches and frisks, followed by Hispanic and White drivers. The ambiguous category (“unknown/other”) was separated to avoid skewing the results, as raw data inspection revealed unusually high frisk rates for missing values—likely a sign of data entry error rather than a trend. These results describe the disparities recorded in the administrative logs, but without controlling for context (such as the reason for the stop), they represent descriptive correlations rather than causal evidence of bias.\n\n\nConclusion\nThis project utilized three distinct subsets of the SOPP database (Pierson et al., 2020) to characterize how traffic stops are recorded across different jurisdictions.\n\nNew York: The racial composition of recorded stops has shifted slightly over time, with the proportion of stops involving Black and Hispanic drivers rising between 2010 and 2018.\nCalifornia: Comparing Long Beach and San Bernardino reveals that while vehicular stops dominate both datasets, agencies differ in their capture of pedestrian interactions.\nFlorida: Analysis of post-stop outcomes highlights that Black drivers have the highest recorded search and frisk rates, though these events remain rare overall.\n\nThese findings underscore the importance of understanding data definitions and reporting conventions before drawing broad conclusions about policing from administrative data.\n\n\nShow code\ndbDisconnect(con_traffic)\n\n\nReferences\nPierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Jenson, D., Shoemaker, A., Ramachandran, V., et al. (2020). “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10.\nStanford Open Policing Project (SOPP) Data. Traffic stop data compiled by the Stanford Open Policing Project and accessed through the Pomona College SQL server. Original dataset and documentation available at: https://openpolicing.stanford.edu. Pierson et al. (2020)."
  },
  {
    "objectID": "AmazonBias.html#the-scenario-automated-hiring-at-amazon",
    "href": "AmazonBias.html#the-scenario-automated-hiring-at-amazon",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "",
    "text": "In 2014, Amazon attempted to automate the talent acquisition process by creating a machine learning algorithm to score job applicants (Dastin, 2018). The company aimed to increase efficiency by developing a tool that could evaluate 100 resumes and identify the top five candidates for recruitment. However, the engineering team eventually disbanded the project in 2015 because the model displayed a systematic bias against women (Dastin, 2018). The tool penalized resumes containing the word “women’s” (e.g., “women’s chess club captain”) and downgraded graduates from two all-women’s colleges (Dastin, 2018)."
  },
  {
    "objectID": "AmazonBias.html#recognizing-and-mitigating-bias",
    "href": "AmazonBias.html#recognizing-and-mitigating-bias",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "Recognizing and Mitigating Bias",
    "text": "Recognizing and Mitigating Bias\nAddressing the Data Value: “Recognize and mitigate bias in ourselves and in the data we use.”\nThe Amazon recruiting tool failed to mitigate bias because the algorithm codified existing gender disparities found in the tech industry. Dastin (2018) reports that the model taught itself that male candidates were preferable because the training data favored terminology used more frequently by men, such as “executed” or “captured.” Conversely, the system penalized resumes containing the word “women’s.” Goodman (2018) argues that such algorithms do not remove human bias but instead “launder” discrimination through software, giving the exclusion of women a veneer of objectivity. By automating the preference for male candidates, Amazon’s tool amplified rather than mitigated the biases present in the hiring process."
  },
  {
    "objectID": "AmazonBias.html#representativeness-of-the-sample",
    "href": "AmazonBias.html#representativeness-of-the-sample",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "Representativeness of the Sample",
    "text": "Representativeness of the Sample\nAddressing the Question: “Who was measured? Are those individuals representative of the people to whom we’d like to generalize?”\nThe data collection process relied on 10 years of resumes previously submitted to Amazon (Dastin, 2018). Because the technology sector is historically male dominated, the majority of the training data came from men. Consequently, the individuals measured were not representative of the ideal applicant pool, but rather representative of Amazon’s past hiring preferences. Goodman (2018) notes that asking software to locate candidates who resemble a current workforce guarantees the reproduction of existing demographics. The lack of female representation in the training data caused the algorithm to treat female gender markers as negative variables, resulting in a failure to generalize fairly across all potential applicants."
  },
  {
    "objectID": "AmazonBias.html#unintended-consequences-and-open-discussion",
    "href": "AmazonBias.html#unintended-consequences-and-open-discussion",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "Unintended Consequences and Open Discussion",
    "text": "Unintended Consequences and Open Discussion\nAddressing the Data Value: “Respect and invite fair criticism while promoting the identification and open discussion of errors, risks, and unintended consequences of our work.”\nWhile Amazon intended to improve efficiency, the unintended consequence was the creation of a discriminatory sorting mechanism. The engineering team identified the risk that the tool would penalize gender-neutral resumes if the algorithm detected other correlations to gender (Dastin, 2018). However, Amazon failed to promote the “open discussion” of these errors. Dastin (2018) reveals that the company quietly disbanded the team and did not publicize the failure until Reuters investigated the matter years later. Rather than using the failure to inform the wider data science community about the risks of algorithmic hiring, Amazon attempted to keep the project’s flaws secret, violating the principle of transparently discussing risks and consequences."
  },
  {
    "objectID": "AmazonBias.html#ethical-implications-and-accountability",
    "href": "AmazonBias.html#ethical-implications-and-accountability",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "Ethical Implications and Accountability",
    "text": "Ethical Implications and Accountability\nAddressing the Data Value: “Consider carefully the ethical implications of choices we make when using data.”\nThe ethical implications of using automated tools for hiring involve a lack of accountability and transparency for the applicants. Goodman (2018) highlights that if Amazon had deployed the tool, rejected applicants would have remained unaware that an algorithm had downgraded their resumes based on gender. Proving discrimination in such cases is “exceedingly difficult” because the decision-making logic is hidden inside proprietary software (Goodman, 2018). The choice to use such a model shifts power away from the individual, who cannot provide informed consent or challenge the decision, and concentrates power within the corporation. This dynamic creates a scenario where ethical violations can occur without detection or recourse."
  },
  {
    "objectID": "AmazonBias.html#summary-power-and-impact",
    "href": "AmazonBias.html#summary-power-and-impact",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "Summary: Power and Impact",
    "text": "Summary: Power and Impact\nThe Amazon recruiting case illustrates how data science can reinforce existing power structures when efficiency is prioritized over equity.\nWho benefits? Amazon stood to benefit financially through the increased efficiency of their hiring pipeline and the reduction of human labor costs.\nWho is harmed? Female applicants were the primary group neglected and harmed. The algorithm actively penalized their credentials based on historical patterns of exclusion in the tech industry.\nThe Power Dynamic: The ethical violations occurred in the interest of profit and efficiency. By relying on historical data without correcting for historical injustice, the project utilized data science to maintain the status quo of male dominance in the technology sector rather than challenging it."
  },
  {
    "objectID": "AmazonBias.html#references",
    "href": "AmazonBias.html#references",
    "title": "Project 4: The Amazon Recruiting Algorithm",
    "section": "References",
    "text": "References\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/\nGoodman, R. (2018, October 16). Why Amazon’s Automated Hiring Tool Discriminated Against Women. ACLU News & Commentary. https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against-women"
  }
]