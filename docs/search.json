[
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization 1",
    "section": "",
    "text": "This dataset contains information on artists, including their names, genders, birth years, and places of origin. Here, I created a visualization the top 5 artist birth years to find the years that produced the most artists. The bar chart at the bottom displays the five most common years of birth in the dataset.\n\n\nlibrary(tidyverse)\n\nartists &lt;- read_csv(\"artists.csv\")\n\ntop_birth_years &lt;- artists |&gt;\n  filter(!is.na(yearOfBirth)) |&gt;\n  group_by(yearOfBirth) |&gt;\n  summarize(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  head(5)\n\ntop_birth_years\n\n# A tibble: 5 × 2\n  yearOfBirth     n\n        &lt;dbl&gt; &lt;int&gt;\n1        1936    49\n2        1930    48\n3        1928    45\n4        1967    45\n5        1938    41\n\n\n\nggplot(top_birth_years, aes(x = reorder(as.factor(yearOfBirth), n), y = n)) +\n  geom_col(fill = \"steelBlue\") +\n  labs(\n    title = \"Top 5 Years of Artist Births\",\n    x = \"Year of Birth\",\n    y = \"Number of Artists\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe TidyTuesday data can be found at: https://github.com/rfordatascience/tidytuesday/tree/main/data/2021/2021-01-12\nThis data comes from the Tate Art Museum (original GitHub link): https://github.com/tategallery/collection"
  },
  {
    "objectID": "chocolate.html",
    "href": "chocolate.html",
    "title": "Data Visualization 2",
    "section": "",
    "text": "This analysis uses the TidyTuesday chocolate ratings dataset which contains reviews many different chocolate bars. I focus on the countries of cocoa bean origin to see which regions appear most often in the data. The plot below shows the ten most common bean origins.\n\nlibrary(tidyverse)\n\nchocolate &lt;- read_csv(\"chocolate.csv\")\n\ntop_bean_countries &lt;- chocolate |&gt;\n  filter(!is.na(country_of_bean_origin)) |&gt;\n  count(country_of_bean_origin, sort = TRUE) |&gt;\n  head(10)\n\ntop_bean_countries\n\n# A tibble: 10 × 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n\n\n\nggplot(top_bean_countries, aes(x = reorder(country_of_bean_origin, n), y = n)) +\n  geom_col(fill = \"chocolate4\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Countries of Cocoa Bean Origin\",\n    x = \"Country of Bean Origin\",\n    y = \"Number of Reviews\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe TidyTuesday data can be found at: https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-01-18\nThis data comes from Flavors of Cacao by way of Georgios and Kelsey. (original link): https://flavorsofcacao.com/chocolate_database.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William Walz",
    "section": "",
    "text": "Hi, this is for my Data Science class! I’ve made two visualizations of random TidyTuesday datasets for Project 1, and two more visualizations for Project 2."
  },
  {
    "objectID": "text-analysis.html",
    "href": "text-analysis.html",
    "title": "Project 2: Text Analysis",
    "section": "",
    "text": "Overview\nI analyze the titles of the Federalist Papers to see if some authors used longer or shorter titles.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\n\ntitles &lt;- read_csv(\"title_author.csv\")\n\nhead(titles)\n\n\n# A tibble: 6 × 4\n  paper author   disputed title                                                 \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                                                 \n1     1 Hamilton no       FEDERALIST No. I. General Introduction                \n2     2 Jay      no       FEDERALIST No. II. Concerning Dangers from Foreign Fo…\n3     3 Jay      no       FEDERALIST No. III. The Same Subject Continued (Conce…\n4     4 Jay      no       FEDERALIST No. IV. The Same Subject Continued (Concer…\n5     5 Jay      no       FEDERALIST No. V. The Same Subject Continued (Concern…\n6     6 Hamilton no       FEDERALIST No. VI. Concerning Dangers from Dissension…\n\n\n\n\nShow code\ntitles_clean &lt;- titles |&gt;\n  mutate(\n    clean_title = str_replace_all(title, \"^FEDERALIST No\\\\. [IVXLC]+\\\\.\\\\s*\", \"\"),\n    clean_title = str_replace_all(clean_title, \"\\\\([^)]*\\\\)\", \"\"),\n    clean_title = str_replace_all(clean_title, \"[[:punct:]]\", \"\"),\n    title_length = str_length(clean_title)\n  )\n\nby_author &lt;- titles_clean |&gt;\n  group_by(author) |&gt;\n  summarise(\n    n_papers = n(),\n    mean_chars = mean(title_length, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(mean_chars))\n\nby_author\n\n\n# A tibble: 3 × 3\n  author   n_papers mean_chars\n  &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;\n1 Madison        29       59.3\n2 Hamilton       51       44.2\n3 Jay             5       31.2\n\n\nThis table summarizes the average cleaned title length (in characters) by author. Madison’s titles are longest on average, while Hamilton and Jay tend to use slightly shorter titles. Madison had 29 papers, Hamilton 51, and Jay 5.\n\n\nShow code\ntitles_clean_copy &lt;- titles_clean\ntitles_starting_with_the &lt;- titles_clean |&gt;\n  select(\n    clean_title, author\n  ) |&gt;\n  mutate(\n    begins_with_the = ifelse(str_detect(clean_title, \"^The(?=\\\\b)\"), 1, 0)\n  ) |&gt;\n  group_by(author\n  ) |&gt;\n  summarize(avg_use_the = mean(begins_with_the) * 100)\n\ntitles_starting_with_the\n\n\n# A tibble: 3 × 2\n  author   avg_use_the\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Hamilton        80.4\n2 Jay             80  \n3 Madison         69.0\n\n\nHere I found titles that began with “The” using str_detect(*). The positive lookahead (?=\\b) made sure that the “The” was matched as a whole word at the start.\n\n\nShow code\nggplot(by_author, aes(x = author, y = mean_chars)) + \n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    x = \"Author\",\n    y = \"Average Number of Characters Per Title\",\n    title = \"Average Federalist Paper Title Length by Author\",\n    subtitle = \"Characters after removing Federalist, prefixes, parentheses\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis chart visualizes the differences in average title length across authors, and puts into perspective the table from by_author. Madison shows the highest mean character count, suggesting that he may have had a preference for longer or more descriptive titles. Jay has the lowest mean count, with Hamilton in the middle of the two.\n\n\nShow code\nggplot(titles_starting_with_the, aes(x = author, y = avg_use_the)) +\n  geom_col(fill = \"darkgreen\", width = 0.75) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    x = \"Author\",\n    y = \"Percent of Titles beginning with 'The'\",\n    title = \"Percent of Titles beginning with 'The' by Author\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the share of each author’s titles that start with “The.” Differences are small, particularly between Hamilton and Jay (with Hamilton having the highest by a very tiny margin), but the pattern displays variation in how authors write their titles.\nData: Nicholas J. Horton, FederalistPapers dataset. CSV used: title_author.csv.\nSource: https://github.com/nicholasjhorton/FederalistPapers"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Project 3: Permutation Test on Life Expectancy (Asia vs Africa)",
    "section": "",
    "text": "Overview\nI test whether countries in Asia and Africa differ in their average life expectancy using data from the Gapminder dataset. I focus on the year 2007 so that each country is represented once (and because that’s the most recent in this dataset). My goal is to see if the observed difference between the two continents could have happened by random chance, or if it reflects a real difference in population health.\nNull hypothesis (H₀): There is no difference in mean life expectancy between Asia and Africa in 2007.\nAlternative hypothesis (Hₐ): The mean life expectancy differs between Asia and Africa in 2007.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(purrr)\n\nkeep_2007 &lt;- gapminder |&gt;\n  filter(year == 2007, continent == \"Asia\" | continent == \"Africa\") |&gt;\n  select(country, continent, lifeExp) |&gt;\n  drop_na()\n\n\nIn the code above, I load the necessary packages and filter the Gapminder dataset to include only countries from Asia and Africa in the year 2007. I then keep only the country name, continent, and life expectancy columns to make a clean dataset for my analysis (remove missing values and focus only on the variables relevant to my research question).\n\n\nVisualize this relationship\n\n\nShow code\nggplot(keep_2007, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot(alpha=0.67) +\n  labs(\n    title = \"Life Expectancy by Continent (2007)\",\n    x = \"Continent\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  guides(fill = \"none\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nAbove is the original data, in boxplots that show life expectancy distributions for the two continents in 2007. Each box is representing the spread of life expectancy within that continent. The plot shows that most Asian countries tend to have higher life expectancies than African countries, but there’s some overlap between the groups as well.\n\n\nObserved Difference (Asia - Africa)\nWhy? In a permutation test, you calculate the difference in means to create a null distribution that represents what you would expect if there was not an actual difference in the groups.\n\n\nShow code\n# first, summary by continent\n\nkeep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  )\n\n\n# A tibble: 2 × 3\n  continent ave_life med_life\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Africa        54.8     52.9\n2 Asia          70.7     72.4\n\n\nShow code\n# observed difference (asia - africa)\n\nobserved_diffs &lt;- keep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  ) |&gt;\n  summarize(\n    obs_ave_diff = diff(ave_life),\n    obs_med_diff = diff(med_life)\n  )\n\nobserved_diffs\n\n\n# A tibble: 1 × 2\n  obs_ave_diff obs_med_diff\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         15.9         19.5\n\n\nHere, I calculate the observed difference in both mean and median life expectancy between Asia and Africa. This step gives me the actual statistic I’m testing against. In this case, the mean difference is 15.92 years and median difference is 19.47 years. These values prove that, on average, people in Asia live substantially longer than those in Africa. In a permutation test, these observed differences act as benchmarks to see how extreme they are relative to differences generated under random shuffling (the null hypothesis).\n\n\nNull sampling distribution\nWhat does this do? It creates the null sampling distribution, whcih shows what differences I’d expect if continent made no real difference tolife expectancy. The function shuffle_sim randomly shuffles the continent labels, recalculates the mean and median (lifeExp) for the shuffled groups, and returns the differences. I repeated this 3,000 times with map to simulate what random differences might look like if there were not a relationship between continent and life expectancy.\n\n\nShow code\n#shuffle_sim\nshuffle_sim &lt;- function(rep, data) {\n  data |&gt;\n    select(continent, lifeExp) |&gt;\n    mutate(continent_perm = sample(continent, replace = FALSE)) |&gt;\n    group_by(continent_perm) |&gt;\n    summarize(\n      ave_perm = mean(lifeExp),\n      med_perm = median(lifeExp),\n      .groups = \"drop\"\n    ) |&gt;\n    summarize(\n      mean_diff_sim = diff(ave_perm),\n      median_diff_sim = diff(med_perm)\n    ) |&gt;\n    mutate(rep = rep)\n}\n\n# null distribution\nset.seed(123)\npermut_stats &lt;- map(1:3000, shuffle_sim, data = keep_2007) |&gt;\n  list_rbind()\n\npermut_stats |&gt; head()\n\n\n# A tibble: 6 × 3\n  mean_diff_sim median_diff_sim   rep\n          &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;\n1          3.29            4.06     1\n2          3.40            6.40     2\n3          1.94            3.20     3\n4         -4.32           -7.19     4\n5          2.38            4.13     5\n6         -2.55           -1.45     6\n\n\n\n\nVisualize! (observed ave diff)\n\n\nShow code\n#observed ave diff\npermut_stats |&gt;\n  ggplot(aes(x = mean_diff_sim)) +\n  geom_histogram(bins = 47) +\n  geom_vline(xintercept = observed_diffs$obs_ave_diff, color = \"red\") +\n               labs(\n                 title = \"Permutation Null Distribution (Mean Difference: Asia - Africa)\",\n                 x = \"Permuted mean differences\",\n                 y = \"Count\"\n               ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThe histogram above shows the null distribution of mean differences generated from the permutations. Each bar represents one possible difference that could occur by random chance if continent didn’t matter. The red line shows the observed mean difference from the real data (15.9 years). Because the red line is far to the right of almost all simulated values, the observed difference is extremely unlikely to be due to chance which provides strong evidence that the continents differ in mean life expectancy (Reject null hypothesis that (lifeExp) in Asia and Africa are not the same).\n\n\nVisualize! (observed med diff)\n\n\nShow code\n#observed med diff\npermut_stats |&gt;\n  ggplot(aes(x = median_diff_sim)) +\n  geom_histogram(bins = 47) +\n         geom_vline(xintercept = observed_diffs$obs_med_diff, color = \"red\") +\n         labs(\n           title = \"Permutation Null Distribution (Median Difference: Asia - Africa)\",\n           x = \"Permuted median differences\",\n           y = \"Count\"\n         ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThis second histogram shows the null distribution of median differences. Again, the red vertical line shows the observed difference in medians (19.5 years). Like the mean comparison, this value lies far beyond the range of most simulated outcomes which suggests that the difference in median life expectancy between Asia and Africa is also statistically meaningful and not arbitrary\n\n\np-value\n\n\nShow code\np_results &lt;- permut_stats |&gt;\n  summarize(\n    p_value_ave = mean(mean_diff_sim &gt; observed_diffs$obs_ave_diff),\n    p_value_med = mean(median_diff_sim &gt; observed_diffs$obs_med_diff)\n  )\n\np_results\n\n\n# A tibble: 1 × 2\n  p_value_ave p_value_med\n        &lt;dbl&gt;       &lt;dbl&gt;\n1           0           0\n\n\nThe p-value shows how likely it is to see a difference this large just by chance. A small p-value means the result is unlikely under the null hypothesis, suggesting a real difference between Asia and Africa.\n\n\nConclusion\nIn this project, I used a permutation simulation to test whether Asia and Africa differ in mean and median life expectancy. After 3,000 random label shuffles, the observed differences were much larger than any values from the null distribution. The p-values were 0, suggesting that the observed gap in life expectancy is absolutely not due to chance. This supports the conclusion that people in Asia generally live longer than those in Africa.\n\n\nData Sources and References:\nGapminder Foundation. (n.d.). Gapminder data. Gapminder Foundation. Retrieved from Gapminder Foundation"
  },
  {
    "objectID": "AmazonBias.html",
    "href": "AmazonBias.html",
    "title": "Project 4: Amazon Tried to Build a Biased Robot",
    "section": "",
    "text": "Amazon Tried to Build a Biased Robot\nIn 2014, Amazon decided it wanted to find the “holy grail” of hiring, as one person told Reuters. They wanted to build an AI that could simply look at 100 resumes and spit out the top five people to hire. The data science part of this was a machine learning model. The ethical issue was that, by 2015, they realized their new system had a bias against women.\nThe problem was the data. According to Reuters, Amazon’s engineers trained the model using 10 years’ worth of resumes that had been submitted to the company, and most of these resumes came from men. The ACLU article points out that this just reflects how the tech industry is already male-dominated. The AI basically taught itself that male candidates were the ideal. The Reuters report also explained that it learned to actually penalize resumes that had the word “women’s” in them (like “women’s chess club captain”) and even downgraded graduates from two all-women’s colleges.\n\n\nMy Ethical Takeaways\n1. An Obvious Bias\nThe biggest and most obvious problem here is bias. The project prompt asks us to “recognize and mitigate bias,” and this is a total failure on that front. As the ACLU article argues, the algorithm wasn’t getting rid of human bias, it was just “laundering it through software.” The AI also learned, as reported by Reuters, that certain verbs men tend to use on their resumes, like “executed” and “captured,” were good, while things associated with women were bad. It just copied the existing bias and made it automatic.\n2. No Representation\nThis leads right to the next problem, representativeness. The prompt asks, “Who was measured? Are those individuals representative…?” The answer here is a hard “no.” The training data wasn’t representative of all good candidates; as Reuters described, it was only representative of Amazon’s past, mostly male workforce. As the ACLU points out in their article, if you just ask software to find more people who look like your current staff, “reproducing the demographics of the existing workforce is virtually guaranteed.” The algorithm didn’t have a good enough picture of what a successful female engineer looked like, so it just decided they were bad.\n3. Unintended Consequence\nThis whole project is a perfect example of “unintended consequences.” Amazon didn’t try to build a sexist AI; according to Reuters, they were just trying to be efficient. The bad consequence was the systematic discrimination. To their credit, Amazon’s team did notice the problem. Reuters mentions they tried to edit the program to make it neutral to specific terms (like “women’s”), but they realized there was “no guarantee” the AI wouldn’t just find other, sneakier ways to be discriminatory. This is why they ended up scrapping the whole project, which I guess counts as “open discussion of… unintended consequences.”\n4. Accountability\nFinally, accountability? The ACLU article brings up a scary point: if Amazon had actually used this tool, how would anyone know? An applicant who got rejected would have no idea it was because an algorithm downgraded her for going to an all-women’s college. As the ACLU argues, it’s “exceedingly difficult” to sue for discrimination in these cases because it’s all hidden inside proprietary software. This lack of transparency means there’s no real accountability for the people who are harmed.\n\n\nWhy Does It Matter?\nIn the end, this whole thing matters because it shows how data science can go really wrong. The intended beneficiary was Amazon, who was looking for profit and efficiency by automating hiring with their AI model. The group harmed was clearly women applying for technical jobs, who were unfairly penalized. This case is a perfect example of how data science can be used in a way that just reinforces existing power structures (in this case, male dominance in tech) instead of fixing them. It’s a good lesson that just because something is “data-driven” doesn’t mean it’s fair.\n\n\nSources\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters.\nGoodman, R. (2018, October 16). Why Amazon’s Automated Hiring Tool Discriminated Against Women. ACLU News & Commentary."
  },
  {
    "objectID": "policingdata.html",
    "href": "policingdata.html",
    "title": "Project 5: Analzying US Traffic Stops",
    "section": "",
    "text": "Using SQL and R, this project explores how traffic-stop data is structured across three jurisdictions in the Stanford Open Policing Project (SOPP). The objective is purely descriptive: to summarize what the dataset records rather than inferring motivations or causal mechanisms behind policing practices. Each section focuses on a specific layer of the data.\nI begin by examining how the racial composition of recorded stops in New York State changes over time, normalizing the data as percentages to allow for consistent comparison across years. Next, I compare Long Beach and San Bernardino to demonstrate how different agencies categorize stop types and how those reporting conventions shape the dataset’s structure. Finally, I analyze search and frisk rates in Florida, looking at how they differ across racial categories while accounting for ambiguous or missing values.\nCollectively, these analyses illustrate how differently agencies record traffic stops, underscoring the importance of understanding data definitions and missingness before drawing broader conclusions.\n\nRacial Distribution of Traffic Stops Over Time: New York State\nTo begin the analysis, I examine how the racial composition of recorded traffic stops in New York State has changed over time. The goal here is descriptive rather than causal. I summarize the data as it appears in the tables without making claims about why these patterns occur or what underlying mechanisms may explain them.\nThe SQL query below groups stops by race and year from 2010-2018 and counts the number of stops in each category. Because the total number of stops varies much across years, I convert these counts into percentages so that each group’s share of all stops can be compared consistently over time.\nIt is important to note that these percentages reflect the distribution within the stop dataset itself, not the demographic composition of New York State’s driving population, nor the population as a whole. Therefore, the results show how the dataset is structured rather than making any statement about disparities or policing practices.\n\n\nShow code\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dbplyr)\n\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\n\nShow code\nSELECT\n  subject_race AS race,\n  COUNT(*) AS n_stops,\n  YEAR(date) AS year\nFROM ny_statewide_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY race, year\nHAVING year BETWEEN 2010 AND 2018;\n\n\nFigure 1. Traffic Stops in New York City by Race and Year as %\n\n\nShow code\nlibrary(ggplot2)\nlibrary(bit64)\nlibrary(dplyr)\nlibrary(scales)\n\nny_stops_clean &lt;- ny_stops |&gt;\n  mutate(across(where(bit64::is.integer64), as.integer))\n\nny_stops_pct &lt;- ny_stops_clean |&gt;\n  group_by(year) |&gt;\n  mutate(\n    total_stops_year = sum(n_stops),\n    pct = n_stops / total_stops_year * 100\n  ) |&gt;\n  ungroup()\n\nggplot(ny_stops_pct, aes(x = year, y = pct, color = race)) +\n  geom_line(linewidth = 1.1) +\n  geom_point(size = 2) +\n  scale_y_continuous(labels = percent_format(scale = 1)) +\n  labs(\n    title = \"Percent of Traffic Stops in New York State by Race and Year\",\n    x = \"Year\",\n    y = \"Percent of Stops\",\n    color = \"Race\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nWhat does this mean?\nThe figure shows how the racial composition of recorded traffic stops in New York State changed between 2010 and 2018. Across all of the years, the majority of stops in this dataset involved individuals classified as white, with their share gradually declining from roughly 78% in 2010 to around 70% by 2018. Other groups make up much smaller proportions of the dataset, though their relative shares show slight increases over time. For example, stops recorded as Black rise from about 10% to approximately 15%, and Hispanic stops increase from roughly 6% to about 9% by 2018.\nIt is important to emphasize that these percentages describe the distribution of stops within the SOPP dataset, not the underlying driving population or the general population of New York State. Without denominators (such as the racial composition of licensed drivers or traffic volume data) we cannot draw conclusions about disparities or exposure to police stops. The figure should therefore be interpreted strictly as a summary of how the stop records themselves are distributed across racial categories over time, rather than as evidence of the causes or consequences of those patterns.\n\n\nMost Common Types of Traffic Stops: Long Beach and San Bernardino, California\nFor the second part of the analysis, I compare how different agencies categorize traffic stops. Both Long Beach and San Bernardino report stop types, but neither provides detailed violation codes of what each specifically means. Instead, each uses broad categories such as vehicular or pedestrian, or other (bikes, scooters, etc.). The following SQL queries count how many times each stop type appears in each agency’s data.\n\n\nShow code\nSELECT\ntype AS stop_type,\nCOUNT(*) AS type_count,\n'Long Beach' AS city\nFROM ca_long_beach_2020_04_01\nGROUP BY stop_type;\n\n\n\n\nShow code\nSELECT\ntype AS stop_type,\nCOUNT(*) AS type_count,\n'San Bernardino' AS city\nFROM ca_san_bernardino_2020_04_01\nGROUP BY stop_type;\n\n\nVisualization: Comparing Stop Type Distributions Across Two Agencies\n\n\nShow code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n# Combination of Long Beach & San Bernardino results\n\ncombined_types &lt;- bind_rows(lb_types, sb_types) |&gt;\n  mutate(\n    stop_type = ifelse(is.na(stop_type) | stop_type == \"\", \"NA\", stop_type)\n    ) |&gt;\n  group_by(city) |&gt;\n  mutate(stop_type = reorder(stop_type, type_count)) |&gt;\n  ungroup()\n\nggplot(combined_types, aes(x = stop_type, y = type_count)) +\n  geom_col(fill = \"#4C72B0\") +\n  coord_flip() +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(~ city, nrow = 1, scales = \"free_x\") +\n  labs(\n    title = \"Distribution of Stop Types Across Two California Agencies\",\n    subtitle = \"Long Beach vs. San Bernardino\",\n    x = \"Stop Type\",\n    y = \"Number of Stops\"\n    ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nWhat does this mean?\nThe two agencies report broadly similar structures in their stop type distributions: both record overwhelmingly more vehicular stops than any other category. This is consistent with the fact that most police–civilian interactions captured in traffic stop databases involve drivers rather than pedestrians. However, Long Beach reports a noticeably larger number of stops classified as pedestrian compared with San Bernardino. This does not necessarily imply differences in enforcement intensity. Rather, it may reflect differences in population density, pedestrian activity, reporting conventions, or simply how each agency defines stop-type categories.\nBoth agencies also include a nontrivial number of stops labeled NA. Because the SOPP documentation notes that some agencies merge multiple non-vehicular stop types or provide incomplete labels, it is likely that the NA category captures a mixture of possibilities, such as bicycle stops, scooter stops, or simply missing or inconsistently coded information. Because these labels do not correspond to a single, clearly defined type, they should not be interpreted substantively.\nOverall, this comparison is descriptive rather than inferential: the figure illustrates how each agency structures and reports its data, not how often particular behaviors occur or how enforcement differs across jurisdictions.\n\n\nSearch Rates and Frisk Rates by Race: Florida\nIn the final part of this project, I analyze how often police searches and frisks occur during traffic stops in Florida, and how those decisions differ by race. The dataset includes search_conducted (whether any search occurred) and frisk_performed (whether a pat-down occurred).\nBefore computing rates, I combined ambiguous racial categories such as NA, “unknown”, and “other” into a single group labeled “unknown/other”. These entries are recorded inconsistently and do not correspond to a clearly defined racial group. In the raw data, for example, the NA category had an unusually high frisk rate, which almost certainly reflects missing or miscoded values rather than a meaningful disparity. Collapsing these ambiguous categories prevents over-interpreting noisy or incomplete data.\nSearch + Frisk Rates (SQL)\n\n\nShow code\nSELECT\nCASE\nWHEN subject_race IS NULL OR subject_race IN ('other', 'unknown', '') THEN 'unknown/other'\nELSE subject_race\nEND AS race,\nCOUNT(*) AS total_stops,\nSUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) AS num_searches,\nSUM(CASE WHEN frisk_performed = 1 THEN 1 ELSE 0 END) AS num_frisk,\n1.0 * SUM(CASE WHEN search_conducted = 1 THEN 1 ELSE 0 END) / COUNT(*) AS search_rate,\n1.0 * SUM(CASE WHEN frisk_performed = 1 THEN 1 ELSE 0 END) / COUNT(*) AS frisk_rate\nFROM fl_statewide_2020_04_01\nGROUP BY race;\n\n\nVisualization: Search & Frisk Rates\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(tidyr)\n\nfl_search_long &lt;- fl_search |&gt;\n  select(race, search_rate, frisk_rate) |&gt;\n  mutate(race = factor(race)) |&gt;\n  pivot_longer(\n    cols = c(search_rate, frisk_rate),\n    names_to = \"metric\",\n    values_to = \"rate\"\n  ) |&gt;\n  mutate(\n    metric = recode(\n      metric,\n      \"search_rate\" = \"Search rate\",\n      \"frisk_rate\"  = \"Frisk rate\"\n    )\n  )\n\nggplot(fl_search_long,\n       aes(x = race, y = rate, fill = race)) +\n  geom_col() +\n  facet_wrap(~ metric, ncol = 2, scales = \"free_x\") +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 0.01)) +\n  labs(\n    title = \"Search and Frisk Rates by Race in Florida\",\n    x = \"Race\",\n    y = \"Share of Stops Involving a Search or Frisk\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.spacing.x = unit(3, \"lines\")\n  )\n\n\n\n\n\n\n\n\n\nWhat does this mean?\nIn this Florida dataset, searches happen more often than frisks, though both are rare compared to the total number of stops. Frisk rates are particularly low (mostly under 0.01%), which is why the axis sits close to zero despite some variation across groups.\nBlack drivers have the highest search rate, followed by Hispanic and White drivers. We see a similar pattern with frisks, but because the actual numbers are so small, these results should be viewed with caution: when an event is this rare, even tiny numerical differences can look significant on a chart.\nAmbiguous categories such as “NA,” “unknown,” and “other” were collapsed because they likely reflect inconsistent or missing data rather than meaningful population groups. In the raw data, for example, the NA category showed an implausibly high frisk rate. This pattern is likely more consistent with miscoding than substantive disparity. Collapsing these categories helps avoid overinterpreting noise and keeps the comparison focused on th distinct groups.\nConclusion\nIn summary, this project used three different slices of the SOPP database to describe how traffic stop data is structured, rather than attempting to prove causal links in policing. In New York, the data shows that the majority of stops involve white drivers, though that share is declining over time while the proportion of Black and Hispanic drivers has seen a modest rise. For Long Beach and San Bernardino, the overwhelming majority of stops are categorized as vehicular. Since pedestrian or ambiguous stops are rare, this suggests the dataset focuses heavily on driver-officer interactions.\nThe Florida analysis looked at post-stop outcomes, finding that while searches and frisks are rare events overall, recorded rates are highest for Black drivers. Throughout the analysis, I treated ambiguous labels like “NA” or “unknown” as likely data noise rather than distinct groups. Ultimately, these findings clarify what the SOPP data can tell us about how agencies record stops, while emphasizing the need to understand reporting differences and data limits before making broader generalizations.\nSources\nStanford Open Policing Project (SOPP) Data. Traffic stop data compiled by the Stanford Open Policing Project and accessed through the Pomona College SQL server. Original dataset and documentation available at: https://openpolicing.stanford.edu. Pierson et al. (2020).\nNew York City Demographic Data. U.S. Census Bureau. New York, New York – Profile (Profile ID 16000US3651000). Demographics can be found at: https://censusreporter.org/profiles/16000US3651000-new-york-ny/\n(Note: CensusReporter provides a visualization layer, but all values originate from the U.S. Census Bureau.)\nReferences"
  }
]