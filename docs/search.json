[
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Visualization 1",
    "section": "",
    "text": "This dataset contains information on artists, including their names, genders, birth years, and places of origin. Here, I created a visualization the top 5 artist birth years to find the years that produced the most artists. The bar chart at the bottom displays the five most common years of birth in the dataset.\n\n\nlibrary(tidyverse)\n\nartists &lt;- read_csv(\"artists.csv\")\n\ntop_birth_years &lt;- artists |&gt;\n  filter(!is.na(yearOfBirth)) |&gt;\n  group_by(yearOfBirth) |&gt;\n  summarize(n = n()) |&gt;\n  arrange(desc(n)) |&gt;\n  head(5)\n\ntop_birth_years\n\n# A tibble: 5 × 2\n  yearOfBirth     n\n        &lt;dbl&gt; &lt;int&gt;\n1        1936    49\n2        1930    48\n3        1928    45\n4        1967    45\n5        1938    41\n\n\n\nggplot(top_birth_years, aes(x = reorder(as.factor(yearOfBirth), n), y = n)) +\n  geom_col(fill = \"steelBlue\") +\n  labs(\n    title = \"Top 5 Years of Artist Births\",\n    x = \"Year of Birth\",\n    y = \"Number of Artists\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe TidyTuesday data can be found at: https://github.com/rfordatascience/tidytuesday/tree/main/data/2021/2021-01-12\nThis data comes from the Tate Art Museum (original GitHub link): https://github.com/tategallery/collection"
  },
  {
    "objectID": "chocolate.html",
    "href": "chocolate.html",
    "title": "Data Visualization 2",
    "section": "",
    "text": "This analysis uses the TidyTuesday chocolate ratings dataset which contains reviews many different chocolate bars. I focus on the countries of cocoa bean origin to see which regions appear most often in the data. The plot below shows the ten most common bean origins.\n\nlibrary(tidyverse)\n\nchocolate &lt;- read_csv(\"chocolate.csv\")\n\ntop_bean_countries &lt;- chocolate |&gt;\n  filter(!is.na(country_of_bean_origin)) |&gt;\n  count(country_of_bean_origin, sort = TRUE) |&gt;\n  head(10)\n\ntop_bean_countries\n\n# A tibble: 10 × 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                253\n 2 Peru                     244\n 3 Dominican Republic       226\n 4 Ecuador                  219\n 5 Madagascar               177\n 6 Blend                    156\n 7 Nicaragua                100\n 8 Bolivia                   80\n 9 Colombia                  79\n10 Tanzania                  79\n\n\n\nggplot(top_bean_countries, aes(x = reorder(country_of_bean_origin, n), y = n)) +\n  geom_col(fill = \"chocolate4\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Countries of Cocoa Bean Origin\",\n    x = \"Country of Bean Origin\",\n    y = \"Number of Reviews\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe TidyTuesday data can be found at: https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-01-18\nThis data comes from Flavors of Cacao by way of Georgios and Kelsey. (original link): https://flavorsofcacao.com/chocolate_database.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William Walz",
    "section": "",
    "text": "Hi, this is for my Data Science class! I’ve made two visualizations of random TidyTuesday datasets for Project 1, and two more visualizations for Project 2."
  },
  {
    "objectID": "text-analysis.html",
    "href": "text-analysis.html",
    "title": "Project 2: Text Analysis",
    "section": "",
    "text": "Overview\nI analyze the titles of the Federalist Papers to see if some authors used longer or shorter titles.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(readr)\n\ntitles &lt;- read_csv(\"title_author.csv\")\n\nhead(titles)\n\n\n# A tibble: 6 × 4\n  paper author   disputed title                                                 \n  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                                                 \n1     1 Hamilton no       FEDERALIST No. I. General Introduction                \n2     2 Jay      no       FEDERALIST No. II. Concerning Dangers from Foreign Fo…\n3     3 Jay      no       FEDERALIST No. III. The Same Subject Continued (Conce…\n4     4 Jay      no       FEDERALIST No. IV. The Same Subject Continued (Concer…\n5     5 Jay      no       FEDERALIST No. V. The Same Subject Continued (Concern…\n6     6 Hamilton no       FEDERALIST No. VI. Concerning Dangers from Dissension…\n\n\n\n\nShow code\ntitles_clean &lt;- titles |&gt;\n  mutate(\n    clean_title = str_replace_all(title, \"^FEDERALIST No\\\\. [IVXLC]+\\\\.\\\\s*\", \"\"),\n    clean_title = str_replace_all(clean_title, \"\\\\([^)]*\\\\)\", \"\"),\n    clean_title = str_replace_all(clean_title, \"[[:punct:]]\", \"\"),\n    title_length = str_length(clean_title)\n  )\n\nby_author &lt;- titles_clean |&gt;\n  group_by(author) |&gt;\n  summarise(\n    n_papers = n(),\n    mean_chars = mean(title_length, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(mean_chars))\n\nby_author\n\n\n# A tibble: 3 × 3\n  author   n_papers mean_chars\n  &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;\n1 Madison        29       59.3\n2 Hamilton       51       44.2\n3 Jay             5       31.2\n\n\nThis table summarizes the average cleaned title length (in characters) by author. Madison’s titles are longest on average, while Hamilton and Jay tend to use slightly shorter titles. Madison had 29 papers, Hamilton 51, and Jay 5.\n\n\nShow code\ntitles_clean_copy &lt;- titles_clean\ntitles_starting_with_the &lt;- titles_clean |&gt;\n  select(\n    clean_title, author\n  ) |&gt;\n  mutate(\n    begins_with_the = ifelse(str_detect(clean_title, \"^The(?=\\\\b)\"), 1, 0)\n  ) |&gt;\n  group_by(author\n  ) |&gt;\n  summarize(avg_use_the = mean(begins_with_the) * 100)\n\ntitles_starting_with_the\n\n\n# A tibble: 3 × 2\n  author   avg_use_the\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Hamilton        80.4\n2 Jay             80  \n3 Madison         69.0\n\n\nHere I found titles that began with “The” using str_detect(*). The positive lookahead (?=\\b) made sure that the “The” was matched as a whole word at the start.\n\n\nShow code\nggplot(by_author, aes(x = author, y = mean_chars)) + \n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    x = \"Author\",\n    y = \"Average Number of Characters Per Title\",\n    title = \"Average Federalist Paper Title Length by Author\",\n    subtitle = \"Characters after removing Federalist, prefixes, parentheses\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis chart visualizes the differences in average title length across authors, and puts into perspective the table from by_author. Madison shows the highest mean character count, suggesting that he may have had a preference for longer or more descriptive titles. Jay has the lowest mean count, with Hamilton in the middle of the two.\n\n\nShow code\nggplot(titles_starting_with_the, aes(x = author, y = avg_use_the)) +\n  geom_col(fill = \"darkgreen\", width = 0.75) +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(\n    x = \"Author\",\n    y = \"Percent of Titles beginning with 'The'\",\n    title = \"Percent of Titles beginning with 'The' by Author\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot shows the share of each author’s titles that start with “The.” Differences are small, particularly between Hamilton and Jay (with Hamilton having the highest by a very tiny margin), but the pattern displays variation in how authors write their titles.\nData: Nicholas J. Horton, FederalistPapers dataset. CSV used: title_author.csv.\nSource: https://github.com/nicholasjhorton/FederalistPapers"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Project 3: Permutation Test on Life Expectancy (Asia vs Africa)",
    "section": "",
    "text": "Overview\nI test whether countries in Asia and Africa differ in their average life expectancy using data from the Gapminder dataset. I focus on the year 2007 so that each country is represented once (and because that’s the most recent in this dataset). My goal is to see if the observed difference between the two continents could have happened by random chance, or if it reflects a real difference in population health.\nNull hypothesis (H₀): There is no difference in mean life expectancy between Asia and Africa in 2007.\nAlternative hypothesis (Hₐ): The mean life expectancy differs between Asia and Africa in 2007.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(purrr)\n\nkeep_2007 &lt;- gapminder |&gt;\n  filter(year == 2007, continent == \"Asia\" | continent == \"Africa\") |&gt;\n  select(country, continent, lifeExp) |&gt;\n  drop_na()\n\n\nIn the code above, I load the necessary packages and filter the Gapminder dataset to include only countries from Asia and Africa in the year 2007. I then keep only the country name, continent, and life expectancy columns to make a clean dataset for my analysis (remove missing values and focus only on the variables relevant to my research question).\n\n\nVisualize this relationship\n\n\nShow code\nggplot(keep_2007, aes(x = continent, y = lifeExp, fill = continent)) +\n  geom_boxplot(alpha=0.67) +\n  labs(\n    title = \"Life Expectancy by Continent (2007)\",\n    x = \"Continent\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  guides(fill = \"none\") + theme_minimal()\n\n\n\n\n\n\n\n\n\nAbove is the original data, in boxplots that show life expectancy distributions for the two continents in 2007. Each box is representing the spread of life expectancy within that continent. The plot shows that most Asian countries tend to have higher life expectancies than African countries, but there’s some overlap between the groups as well.\n\n\nObserved Difference (Asia - Africa)\nWhy? In a permutation test, you calculate the difference in means to create a null distribution that represents what you would expect if there was not an actual difference in the groups.\n\n\nShow code\n# first, summary by continent\n\nkeep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  )\n\n\n# A tibble: 2 × 3\n  continent ave_life med_life\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 Africa        54.8     52.9\n2 Asia          70.7     72.4\n\n\nShow code\n# observed difference (asia - africa)\n\nobserved_diffs &lt;- keep_2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    ave_life = mean(lifeExp),\n    med_life = median(lifeExp),\n    .groups = \"drop\"\n  ) |&gt;\n  summarize(\n    obs_ave_diff = diff(ave_life),\n    obs_med_diff = diff(med_life)\n  )\n\nobserved_diffs\n\n\n# A tibble: 1 × 2\n  obs_ave_diff obs_med_diff\n         &lt;dbl&gt;        &lt;dbl&gt;\n1         15.9         19.5\n\n\nHere, I calculate the observed difference in both mean and median life expectancy between Asia and Africa. This step gives me the actual statistic I’m testing against. In this case, the mean difference is 15.92 years and median difference is 19.47 years. These values prove that, on average, people in Asia live substantially longer than those in Africa. In a permutation test, these observed differences act as benchmarks to see how extreme they are relative to differences generated under random shuffling (the null hypothesis).\n\n\nNull sampling distribution\nWhat does this do? It creates the null sampling distribution, whcih shows what differences I’d expect if continent made no real difference tolife expectancy. The function shuffle_sim randomly shuffles the continent labels, recalculates the mean and median (lifeExp) for the shuffled groups, and returns the differences. I repeated this 3,000 times with map to simulate what random differences might look like if there were not a relationship between continent and life expectancy.\n\n\nShow code\n#shuffle_sim\nshuffle_sim &lt;- function(rep, data) {\n  data |&gt;\n    select(continent, lifeExp) |&gt;\n    mutate(continent_perm = sample(continent, replace = FALSE)) |&gt;\n    group_by(continent_perm) |&gt;\n    summarize(\n      ave_perm = mean(lifeExp),\n      med_perm = median(lifeExp),\n      .groups = \"drop\"\n    ) |&gt;\n    summarize(\n      mean_diff_sim = diff(ave_perm),\n      median_diff_sim = diff(med_perm)\n    ) |&gt;\n    mutate(rep = rep)\n}\n\n# null distribution\nset.seed(123)\npermut_stats &lt;- map(1:3000, shuffle_sim, data = keep_2007) |&gt;\n  list_rbind()\n\npermut_stats |&gt; head()\n\n\n# A tibble: 6 × 3\n  mean_diff_sim median_diff_sim   rep\n          &lt;dbl&gt;           &lt;dbl&gt; &lt;int&gt;\n1          3.29            4.06     1\n2          3.40            6.40     2\n3          1.94            3.20     3\n4         -4.32           -7.19     4\n5          2.38            4.13     5\n6         -2.55           -1.45     6\n\n\n\n\nVisualize! (observed ave diff)\n\n\nShow code\n#observed ave diff\npermut_stats |&gt;\n  ggplot(aes(x = mean_diff_sim)) +\n  geom_histogram(bins = 47) +\n  geom_vline(xintercept = observed_diffs$obs_ave_diff, color = \"red\") +\n               labs(\n                 title = \"Permutation Null Distribution (Mean Difference: Asia - Africa)\",\n                 x = \"Permuted mean differences\",\n                 y = \"Count\"\n               ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThe histogram above shows the null distribution of mean differences generated from the permutations. Each bar represents one possible difference that could occur by random chance if continent didn’t matter. The red line shows the observed mean difference from the real data (15.9 years). Because the red line is far to the right of almost all simulated values, the observed difference is extremely unlikely to be due to chance which provides strong evidence that the continents differ in mean life expectancy (Reject null hypothesis that (lifeExp) in Asia and Africa are not the same).\n\n\nVisualize! (observed med diff)\n\n\nShow code\n#observed med diff\npermut_stats |&gt;\n  ggplot(aes(x = median_diff_sim)) +\n  geom_histogram(bins = 47) +\n         geom_vline(xintercept = observed_diffs$obs_med_diff, color = \"red\") +\n         labs(\n           title = \"Permutation Null Distribution (Median Difference: Asia - Africa)\",\n           x = \"Permuted median differences\",\n           y = \"Count\"\n         ) + theme_minimal()\n\n\n\n\n\n\n\n\n\nThis second histogram shows the null distribution of median differences. Again, the red vertical line shows the observed difference in medians (19.5 years). Like the mean comparison, this value lies far beyond the range of most simulated outcomes which suggests that the difference in median life expectancy between Asia and Africa is also statistically meaningful and not arbitrary\n\n\np-value\n\n\nShow code\np_results &lt;- permut_stats |&gt;\n  summarize(\n    p_value_ave = mean(mean_diff_sim &gt; observed_diffs$obs_ave_diff),\n    p_value_med = mean(median_diff_sim &gt; observed_diffs$obs_med_diff)\n  )\n\np_results\n\n\n# A tibble: 1 × 2\n  p_value_ave p_value_med\n        &lt;dbl&gt;       &lt;dbl&gt;\n1           0           0\n\n\nThe p-value shows how likely it is to see a difference this large just by chance. A small p-value means the result is unlikely under the null hypothesis, suggesting a real difference between Asia and Africa.\n\n\nConclusion\nIn this project, I used a permutation simulation to test whether Asia and Africa differ in mean and median life expectancy. After 3,000 random label shuffles, the observed differences were much larger than any values from the null distribution. The p-values were 0, suggesting that the observed gap in life expectancy is absolutely not due to chance. This supports the conclusion that people in Asia generally live longer than those in Africa.\n\n\nData Sources and References:\nGapminder Foundation. (n.d.). Gapminder data. Gapminder Foundation. Retrieved from Gapminder Foundation"
  },
  {
    "objectID": "AmazonBias.html",
    "href": "AmazonBias.html",
    "title": "Project 4: Amazon Tried to Build a Biased Robot",
    "section": "",
    "text": "Amazon Tried to Build a Biased Robot\nIn 2014, Amazon decided it wanted to find the “holy grail” of hiring, as one person told Reuters. They wanted to build an AI that could simply look at 100 resumes and spit out the top five people to hire. The data science part of this was a machine learning model. The ethical issue was that, by 2015, they realized their new system had a bias against women.\nThe problem was the data. According to Reuters, Amazon’s engineers trained the model using 10 years’ worth of resumes that had been submitted to the company, and most of these resumes came from men. The ACLU article points out that this just reflects how the tech industry is already male-dominated. The AI basically taught itself that male candidates were the ideal. The Reuters report also explained that it learned to actually penalize resumes that had the word “women’s” in them (like “women’s chess club captain”) and even downgraded graduates from two all-women’s colleges.\n\n\nMy Ethical Takeaways\n1. An Obvious Bias\nThe biggest and most obvious problem here is bias. The project prompt asks us to “recognize and mitigate bias,” and this is a total failure on that front. As the ACLU article argues, the algorithm wasn’t getting rid of human bias, it was just “laundering it through software.” The AI also learned, as reported by Reuters, that certain verbs men tend to use on their resumes, like “executed” and “captured,” were good, while things associated with women were bad. It just copied the existing bias and made it automatic.\n2. No Representation\nThis leads right to the next problem, representativeness. The prompt asks, “Who was measured? Are those individuals representative…?” The answer here is a hard “no.” The training data wasn’t representative of all good candidates; as Reuters described, it was only representative of Amazon’s past, mostly male workforce. As the ACLU points out in their article, if you just ask software to find more people who look like your current staff, “reproducing the demographics of the existing workforce is virtually guaranteed.” The algorithm didn’t have a good enough picture of what a successful female engineer looked like, so it just decided they were bad.\n3. Unintended Consequence\nThis whole project is a perfect example of “unintended consequences.” Amazon didn’t try to build a sexist AI; according to Reuters, they were just trying to be efficient. The bad consequence was the systematic discrimination. To their credit, Amazon’s team did notice the problem. Reuters mentions they tried to edit the program to make it neutral to specific terms (like “women’s”), but they realized there was “no guarantee” the AI wouldn’t just find other, sneakier ways to be discriminatory. This is why they ended up scrapping the whole project, which I guess counts as “open discussion of… unintended consequences.”\n4. Accountability\nFinally, accountability? The ACLU article brings up a scary point: if Amazon had actually used this tool, how would anyone know? An applicant who got rejected would have no idea it was because an algorithm downgraded her for going to an all-women’s college. As the ACLU argues, it’s “exceedingly difficult” to sue for discrimination in these cases because it’s all hidden inside proprietary software. This lack of transparency means there’s no real accountability for the people who are harmed.\n\n\nWhy Does It Matter?\nIn the end, this whole thing matters because it shows how data science can go really wrong. The intended beneficiary was Amazon, who was looking for profit and efficiency by automating hiring with their AI model. The group harmed was clearly women applying for technical jobs, who were unfairly penalized. This case is a perfect example of how data science can be used in a way that just reinforces existing power structures (in this case, male dominance in tech) instead of fixing them. It’s a good lesson that just because something is “data-driven” doesn’t mean it’s fair.\n\n\nSources\nDastin, J. (2018, October 10). Amazon scraps secret AI recruiting tool that showed bias against women. Reuters.\nGoodman, R. (2018, October 16). Why Amazon’s Automated Hiring Tool Discriminated Against Women. ACLU News & Commentary."
  }
]