---
title: "Project 4: Amazon Tried to Build a Biased Robot"
author: William Walz
date: November 12, 2025
format: 
  html:
    code-fold: true
    code-summary: "Show code"
execute: 
  warning: false
  message: false
---

### Amazon Tried to Build a Biased Robot

In 2014, Amazon decided it wanted to find the "holy grail" of hiring, as one person told Reuters. They wanted to build an AI that could simply look at 100 resumes and spit out the top five people to hire. The data science part of this was a machine learning model. The ethical issue was that, by 2015, they realized their new system had a bias against women.

The problem was the data. Amazon's engineers trained the model using 10 years' worth of resumes that had been submitted to the company. According to Reuters, most of these resumes came from men, showing the tech industry was male-dominated. The AI basically taught itself that male candidates were the ideal, the standard. It learned to actually penalize resumes that had the word "women's" in them (like "women's chess club captain") and even downgraded graduates from two all-women's colleges.

### My Ethical Takeaways

**1. An Obvious Bias**

The biggest and most obvious problem here is bias. The project prompt asks us to "recognize and mitigate bias," and this is a total failure on that front. The ACLU article makes a great point that the algorithm wasn't getting rid of human bias, it was just "laundering it through software." The AI learned that certain verbs men tend to use on their resumes, like "executed" and "captured," were good, while things associated with women were bad (Reuters). It just copied the existing bias and made it automatic.

**2. No Representation**

This leads right to the next problem of representativeness. The prompt asks, "Who was measured? Are those individuals representative of...?" The answer here is a hard "no." The training data wasn't representative of all good candidates, but only representative of Amazon's past, mostly a male workforce. As the ACLU points out, if you just ask software to find more people who look like your current staff, "reproducing the demographics of the existing workforce is virtually guaranteed." The algorithm didn't have a picture good enough to know what a successful female engineer looked like, so it just decided they were bad.

**3. Unintended Consequence**

This whole project is a perfect example of "unintended consequences." Amazon didn't try to build a sexist AI; they were just trying to be efficient (Reuters). The bad consequence was the systematic discrimination. To their credit, Amazon's team did notice the problem. Reuters mentions they tried to edit the program to make it neutral to specific terms (like "women's"), but they realized there was "no guarantee" the AI wouldn't just find other, sneakier ways to be discriminatory. This is why they ended up scrapping the whole project, which I guess counts as "open discussion of... unintended consequences."

**4. Accountability**

Lastly, accountability? The ACLU article brings up a scary point: if Amazon had actually used this tool, how would anyone know for sure? An applicant who got rejected would have no idea it was because an algorithm downgraded her for going to an all-women's college. The ACLU says it's "exceedingly difficult" to sue for discrimination in these cases because it's all hidden inside proprietary software. This lack of transparency means there's no real accountability for the people who are harmed, which could result in much bias.

### Why Does It Matter?

In the end, this whole thing matters because it shows how data science can go really wrong. The intended beneficiary was Amazon, who was looking for profit and efficiency by automating hiring with their AI model. The group harmed was clearly women applying for technical jobs, who were unfairly penalized. This case is a perfect example of how data science can be used in a way that just reinforces existing power structures (in this case, male dominance in tech) instead of fixing them. It's a good lesson that just because something is "data-driven" doesn't mean it's fair.

### Sources

Dastin, J. (2018, October 10). *[Amazon scraps secret AI recruiting tool that showed bias against women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).* Reuters.

Goodman, R. (2018, October 16). *[Why Amazonâ€™s Automated Hiring Tool Discriminated Against Women](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against).* ACLU News & Commentary.


