---
title: "Project 4: Amazon Tried to Build a Biased Robot"
author: William Walz
date: November 12, 2025
format: 
  html:
    code-fold: true
    code-summary: "Show code"
execute: 
  warning: false
  message: false
---

### Amazon Tried to Build a Biased Robot

In 2014, Amazon decided it wanted to find the "holy grail" of hiring, as one person told Reuters. They wanted to build an AI that could simply look at 100 resumes and spit out the top five people to hire. The data science part of this was a machine learning model. The ethical issue was that, by 2015, they realized their new system had a bias against women.

The problem was the data. According to Reuters, Amazon's engineers trained the model using 10 years' worth of resumes that had been submitted to the company, and most of these resumes came from men. The ACLU article points out that this just reflects how the tech industry is already male-dominated. The AI basically taught itself that male candidates were the ideal. The Reuters report also explained that it learned to actually penalize resumes that had the word "women's" in them (like "women's chess club captain") and even downgraded graduates from two all-women's colleges.

### My Ethical Takeaways

**1. An Obvious Bias**

The biggest and most obvious problem here is bias. The project prompt asks us to "recognize and mitigate bias," and this is a total failure on that front. As the ACLU article argues, the algorithm wasn't getting rid of human bias, it was just "laundering it through software." The AI also learned, as reported by Reuters, that certain verbs men tend to use on their resumes, like "executed" and "captured," were good, while things associated with women were bad. It just copied the existing bias and made it automatic.

**2. No Representation**

This leads right to the next problem, representativeness. The prompt asks, "Who was measured? Are those individuals representative...?" The answer here is a hard "no." The training data wasn't representative of all good candidates; as Reuters described, it was only representative of Amazon's past, mostly male workforce. As the ACLU points out in their article, if you just ask software to find more people who look like your current staff, "reproducing the demographics of the existing workforce is virtually guaranteed." The algorithm didn't have a good enough picture of what a successful female engineer looked like, so it just decided they were bad.

**3. Unintended Consequence**

This whole project is a perfect example of "unintended consequences." Amazon didn't try to build a sexist AI; according to Reuters, they were just trying to be efficient. The bad consequence was the systematic discrimination. To their credit, Amazon's team did notice the problem. Reuters mentions they tried to edit the program to make it neutral to specific terms (like "women's"), but they realized there was "no guarantee" the AI wouldn't just find other, sneakier ways to be discriminatory. This is why they ended up scrapping the whole project, which I guess counts as "open discussion of... unintended consequences."

**4. Accountability**

Finally, accountability? The ACLU article brings up a scary point: if Amazon had actually used this tool, how would anyone know? An applicant who got rejected would have no idea it was because an algorithm downgraded her for going to an all-women's college. As the ACLU argues, it's "exceedingly difficult" to sue for discrimination in these cases because it's all hidden inside proprietary software. This lack of transparency means there's no real accountability for the people who are harmed.

### Why Does It Matter?

In the end, this whole thing matters because it shows how data science can go really wrong. The intended beneficiary was Amazon, who was looking for profit and efficiency by automating hiring with their AI model. The group harmed was clearly women applying for technical jobs, who were unfairly penalized. This case is a perfect example of how data science can be used in a way that just reinforces existing power structures (in this case, male dominance in tech) instead of fixing them. It's a good lesson that just because something is "data-driven" doesn't mean it's fair.

### Sources

Dastin, J. (2018, October 10). [*Amazon scraps secret AI recruiting tool that showed bias against women*](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)*.* Reuters.

Goodman, R. (2018, October 16). [*Why Amazonâ€™s Automated Hiring Tool Discriminated Against Women*](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against)*.* ACLU News & Commentary.
